<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Linlin Cheng - PhD Researcher</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            line-height: 1.6;
            color: #333;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            min-height: 100vh;
        }

        .container {
            max-width: 1200px;
            margin: 0 auto;
            background: white;
            box-shadow: 0 0 30px rgba(0,0,0,0.1);
            min-height: 100vh;
        }

        header {
            background: linear-gradient(135deg, #2c3e50 0%, #3498db 100%);
            color: white;
            padding: 2rem 0;
            text-align: center;
            position: relative;
            overflow: hidden;
        }

        header::before {
            content: '';
            position: absolute;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background: url('data:image/svg+xml,<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 100 100"><defs><pattern id="grid" width="10" height="10" patternUnits="userSpaceOnUse"><path d="M 10 0 L 0 0 0 10" fill="none" stroke="rgba(255,255,255,0.1)" stroke-width="0.5"/></pattern></defs><rect width="100" height="100" fill="url(%23grid)"/></svg>');
            opacity: 0.3;
        }

        .header-content {
            position: relative;
            z-index: 1;
        }

        .profile-img {
            width: 150px;
            height: 150px;
            border-radius: 50%;
            border: 4px solid rgba(255,255,255,0.3);
            margin: 0 auto 1rem;
            background: linear-gradient(45deg, #f39c12, #e67e22);
            display: flex;
            align-items: center;
            justify-content: center;
            font-size: 3rem;
            font-weight: bold;
            color: white;
            text-shadow: 2px 2px 4px rgba(0,0,0,0.3);
        }

        h1 {
            font-size: 2.5rem;
            margin-bottom: 0.5rem;
            text-shadow: 2px 2px 4px rgba(0,0,0,0.3);
        }

        .subtitle {
            font-size: 1.2rem;
            opacity: 0.9;
            margin-bottom: 1rem;
        }

        .contact-info {
            display: flex;
            justify-content: center;
            gap: 2rem;
            flex-wrap: wrap;
            margin-bottom: 1rem;
            font-size: 0.9rem;
        }

        .contact-info a {
            color: rgba(255,255,255,0.9);
            text-decoration: none;
            transition: color 0.3s ease;
        }

        .contact-info a:hover {
            color: white;
        }

        nav {
            background: rgba(0,0,0,0.1);
            padding: 1rem 0;
        }

        nav ul {
            list-style: none;
            display: flex;
            justify-content: center;
            gap: 2rem;
            flex-wrap: wrap;
        }

        nav a {
            color: white;
            text-decoration: none;
            padding: 0.5rem 1rem;
            border-radius: 25px;
            transition: all 0.3s ease;
            background: rgba(255,255,255,0.1);
            backdrop-filter: blur(10px);
        }

        nav a:hover, nav a.active {
            background: rgba(255,255,255,0.2);
            transform: translateY(-2px);
            box-shadow: 0 4px 15px rgba(0,0,0,0.2);
        }

        main {
            padding: 3rem 2rem;
        }

        .section {
            margin-bottom: 4rem;
            opacity: 0;
            transform: translateY(30px);
            animation: fadeInUp 0.6s ease forwards;
        }

        .section:nth-child(2) { animation-delay: 0.2s; }
        .section:nth-child(3) { animation-delay: 0.4s; }
        .section:nth-child(4) { animation-delay: 0.6s; }
        .section:nth-child(5) { animation-delay: 0.8s; }

        @keyframes fadeInUp {
            to {
                opacity: 1;
                transform: translateY(0);
            }
        }

        h2 {
            color: #2c3e50;
            font-size: 2rem;
            margin-bottom: 1.5rem;
            position: relative;
            padding-bottom: 0.5rem;
        }

        h2::after {
            content: '';
            position: absolute;
            bottom: 0;
            left: 0;
            width: 60px;
            height: 3px;
            background: linear-gradient(135deg, #3498db, #2c3e50);
            border-radius: 2px;
        }

        .about-grid {
            display: grid;
            grid-template-columns: 1fr 1fr;
            gap: 2rem;
            margin-bottom: 2rem;
        }

        .about-text {
            font-size: 1.1rem;
            line-height: 1.8;
        }

        .stats {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(120px, 1fr));
            gap: 1rem;
        }

        .stat-card {
            background: linear-gradient(135deg, #74b9ff, #0984e3);
            color: white;
            padding: 1.5rem;
            border-radius: 15px;
            text-align: center;
            transform: translateY(0);
            transition: transform 0.3s ease;
        }

        .stat-card:hover {
            transform: translateY(-5px);
        }

        .stat-number {
            font-size: 2rem;
            font-weight: bold;
            display: block;
        }

        .projects-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(350px, 1fr));
            gap: 2rem;
        }

        .project-card {
            background: white;
            border-radius: 20px;
            overflow: hidden;
            box-shadow: 0 10px 30px rgba(0,0,0,0.1);
            transition: all 0.3s ease;
            border: 1px solid #e0e0e0;
        }

        .project-card:hover {
            transform: translateY(-10px);
            box-shadow: 0 20px 40px rgba(0,0,0,0.15);
        }

        .project-header {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 1.5rem;
            position: relative;
            overflow: hidden;
        }

        .project-header::before {
            content: '';
            position: absolute;
            top: -50%;
            right: -50%;
            width: 100%;
            height: 100%;
            background: radial-gradient(circle, rgba(255,255,255,0.1) 0%, transparent 70%);
            animation: float 6s ease-in-out infinite;
        }

        @keyframes float {
            0%, 100% { transform: translate(0, 0) rotate(0deg); }
            50% { transform: translate(-20px, -20px) rotate(180deg); }
        }

        .project-content {
            padding: 1.5rem;
        }

        .project-title {
            font-size: 1.3rem;
            font-weight: bold;
            margin-bottom: 0.5rem;
            position: relative;
            z-index: 1;
        }

        .project-description {
            margin-bottom: 1rem;
            color: #666;
            line-height: 1.6;
        }

        .tech-tags {
            display: flex;
            flex-wrap: wrap;
            gap: 0.5rem;
            margin-bottom: 1rem;
        }

        .tech-tag {
            background: linear-gradient(135deg, #74b9ff, #0984e3);
            color: white;
            padding: 0.3rem 0.8rem;
            border-radius: 20px;
            font-size: 0.8rem;
            font-weight: 500;
        }

        .project-links {
            display: flex;
            gap: 1rem;
        }

        .btn {
            padding: 0.7rem 1.5rem;
            border: none;
            border-radius: 25px;
            text-decoration: none;
            font-weight: 500;
            transition: all 0.3s ease;
            cursor: pointer;
            display: inline-flex;
            align-items: center;
            gap: 0.5rem;
        }

        .btn-primary {
            background: linear-gradient(135deg, #00b894, #00a085);
            color: white;
        }

        .btn-secondary {
            background: transparent;
            color: #2c3e50;
            border: 2px solid #2c3e50;
        }

        .btn:hover {
            transform: translateY(-2px);
            box-shadow: 0 5px 15px rgba(0,0,0,0.2);
        }

        .publications {
            background: #f8f9fa;
            padding: 2rem;
            border-radius: 15px;
            margin-bottom: 2rem;
        }

        .publication-item {
            background: white;
            padding: 1.5rem;
            border-radius: 10px;
            margin-bottom: 1rem;
            border-left: 4px solid #3498db;
            transition: transform 0.3s ease;
        }

        .publication-item:hover {
            transform: translateX(5px);
        }

        .pub-title {
            font-weight: bold;
            color: #2c3e50;
            margin-bottom: 0.5rem;
        }

        .pub-authors {
            color: #666;
            font-style: italic;
            margin-bottom: 0.3rem;
        }

        .pub-venue {
            color: #3498db;
            font-weight: 500;
        }

        .pub-status {
            background: #e74c3c;
            color: white;
            padding: 0.2rem 0.5rem;
            border-radius: 10px;
            font-size: 0.8rem;
            margin-left: 0.5rem;
        }

        .pub-status.published {
            background: #27ae60;
        }

        .contact-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(250px, 1fr));
            gap: 2rem;
        }

        .contact-card {
            background: linear-gradient(135deg, #a29bfe, #6c5ce7);
            color: white;
            padding: 2rem;
            border-radius: 20px;
            text-align: center;
            transition: transform 0.3s ease;
        }

        .contact-card:hover {
            transform: scale(1.05);
        }

        .contact-icon {
            font-size: 2rem;
            margin-bottom: 1rem;
        }

        footer {
            background: #2c3e50;
            color: white;
            text-align: center;
            padding: 2rem;
        }

        @media (max-width: 768px) {
            .about-grid {
                grid-template-columns: 1fr;
            }
            
            nav ul {
                gap: 1rem;
            }
            
            .projects-grid {
                grid-template-columns: 1fr;
            }
            
            h1 {
                font-size: 2rem;
            }

            .contact-info {
                gap: 1rem;
            }
        }

        .demo-frame {
            width: 100%;
            height: 300px;
            border: none;
            border-radius: 10px;
            background: #f0f0f0;
            margin: 1rem 0;
            display: flex;
            align-items: center;
            justify-content: center;
            color: #666;
            font-style: italic;
            flex-direction: column;
            text-align: center;
        }

        .current-status {
            background: linear-gradient(135deg, #fd79a8, #e84393);
            color: white;
            padding: 1rem 2rem;
            border-radius: 15px;
            text-align: center;
            margin-bottom: 2rem;
            font-weight: 500;
        }
    </style>
</head>
<body>
    <div class="container">
        <header>
            <div class="header-content">
<!--                <div class="profile-img">LC</div>-->
                <div class="profile-img">
                    <img src="images/profile-photo.jpg" alt="Linlin Cheng"
                         style="width: 100%; height: 100%; border-radius: 50%; object-fit: cover;">
                </div>
                <h1>Linlin Cheng</h1>
                <p class="subtitle">PhD Candidate in Computer Science | Eye Gaze in Human-Robot Interaction</p>
                
                <div class="contact-info">
                    <a href="mailto:linlincheng.research@gmail.com">üìß linlincheng.research@gmail.com</a>
                    <a href="https://github.com/LinReseach" target="_blank">üêô GitHub</a>
                    <a href="tel:+31644600165">üì± +31 644600165</a>
                </div>
                
                <nav>
                    <ul>
                        <li><a href="#about" class="active">About</a></li>
                        <li><a href="#research">Research</a></li>
                        <li><a href="#projects">Projects</a></li>
                        <li><a href="#publications">Publications</a></li>
                        <li><a href="#contact">Contact</a></li>
                    </ul>
                </nav>
            </div>
        </header>

        <main>
            <section id="about" class="section">
                <div class="current-status">
                    üéì Currently pursuing PhD at Vrije Universiteit Amsterdam (2021-Present) | ü§ñ Specializing in Eye Gaze for Human-Robot Interaction
                </div>
                
                <h2>About Me</h2>
                <div class="about-text">
                    <p>I have always been deeply curious, a trait that led me to pursue a PhD. As a child, I was captivated by mysteries like UFOs and phenomena such as the two-photon double-slit experiment, which hints at light's intriguing behavior under observation. This passion for the unknown inspired my journey through computer science and robotics, culminating in my current PhD at Vrije Universiteit Amsterdam, where I specialize in human-robot interaction and gaze estimation.</p>
                    
     
                </div>
            </section>

            <section id="research" class="section">
                <h2>Research Projects</h2>
                
                <div style="margin-bottom: 3rem;">
                    <h3 style="color: #3498db; margin-bottom: 1.5rem; font-size: 1.5rem;">üéì PhD Research (2021-Present) - Vrije Universiteit Amsterdam</h3>
                    <div style="background: #f8f9fa; padding: 2rem; border-radius: 15px; border-left: 4px solid #3498db;">
                        <h4 style="color: #2c3e50; margin-bottom: 1rem;">Eye Gaze in Human-Robot Interaction</h4>
                        <p><strong>Gaze Model Evaluation:</strong> Benchmarked appearance-based gaze estimation models (L2CS) on Pepper robot across varying distances and camera resolutions, achieving up to 18.6% improvement in accuracy and precision.</p>
                        
                        <p><strong>Annotation Pipeline Development:</strong> Created semi-automated systems for annotating gaze targets in video recordings with aggregation algorithms for frame-to-event-level conversion.</p>
                        
                        <p><strong>Validation Studies:</strong> Conducted comparative analysis between 4K camera-based gaze predictions and Pupil Labs eye-tracker data, demonstrating strong alignment in average gaze patterns.</p>
                        
                        <p><strong>Multimodal Interaction Research:</strong> Investigated incongruence effects in speech, gesture, and tablet cues, revealing the dominant distraction effect of tablet-based interfaces in HRI scenarios.</p>
                        
                        <p><strong>Gaze-Based Interaction (Ongoing):</strong> Developing a lightweight, real-time gaze tracking system for Pepper to detect task completion using gaze shifts as natural signals for autonomous task advancement.</p>
                    </div>
                </div>

                <div style="margin-bottom: 3rem;">
                    <h3 style="color: #e74c3c; margin-bottom: 1.5rem; font-size: 1.5rem;">üî¨ Master's Research (2018-2021) - Southeast University</h3>
                    <div style="background: #f8f9fa; padding: 2rem; border-radius: 15px; border-left: 4px solid #e74c3c;">
                        <h4 style="color: #2c3e50; margin-bottom: 1rem;">Multimodal Interaction Technology for Cooperative Control of Multi-agent Systems</h4>
                        <p><strong>Project Implementer:</strong> National Key Research and Development Program of China</p>
                        
                        <p><strong>Force-Feedback Haptic Interfaces:</strong> Designed and analyzed haptic interfaces and formation control strategies for mobile robot teams with simulation platforms in ROS-Gazebo.</p>
                        
                        <p><strong>Autonomous Navigation:</strong> Implemented autonomous navigation and obstacle avoidance systems for multi-robot coordination.</p>
                        
                        <p><strong>Bilateral Teleoperation System:</strong> Developed a system combining eye-tracking and hand input for team formation control with task-layered control framework separating user-controlled main tasks from autonomous subtasks.</p>
                        
                        <p><strong>Patent Achievement:</strong> Co-authored patent for semi-autonomous control method for formation and navigation of multiple mobile robots based on force feedback and eye tracking.</p>
                    </div>
                </div>

                <div>
                    <h3 style="color: #27ae60; margin-bottom: 1.5rem; font-size: 1.5rem;">üìö Bachelor's Research (2016-2017) - North University of China</h3>
                    <div style="background: #f8f9fa; padding: 2rem; border-radius: 15px; border-left: 4px solid #27ae60;">
                        <h4 style="color: #2c3e50; margin-bottom: 1rem;">Speech Signal Analysis Method and Software Design</h4>
                        <p><strong>Thesis Project:</strong> Bachelor graduation design focusing on advanced audio processing techniques</p>
                        
                        <p><strong>Algorithm Improvement:</strong> Enhanced algorithms for monaural separation of singing voice based on Robust Principal Component Analysis (RPCA).</p>
                        
                        <p><strong>Separation Analysis:</strong> Analyzed separation effects of RPCA algorithms on various music types to optimize performance across different audio content.</p>
                        
                        <p><strong>Software Development:</strong> Designed and implemented GUI-based software for voice processing applications in MATLAB, providing user-friendly interface for audio analysis tasks.</p>
                        
                        <p><strong>Recognition:</strong> Received Excellent Graduation Design Award in 2017 National Graduation Design Measurement & Control Technique & Instrument competition.</p>
                    </div>
                </div>
            </section>

            <section id="projects" class="section">
                <h2>Research Projects & Demos</h2>
                <div class="projects-grid">
                    <div class="project-card">
                        <div class="project-header">
                            <h3 class="project-title">üëÅÔ∏è Gaze Following for Social Robots</h3>
                        </div>
                        <div class="project-content">
                            <p class="project-description">A demonstration application showcasing the feasibility of using state-of-the-art gaze estimation models in HRI scenarios. The system simulates a gaze following task where robots follow users' gaze to look at the same object, enabling shared attention and interest communication in environments like stores.</p>
                            
<!--                            <div class="demo-frame">-->
<!--                                <strong>Gaze Following Demo</strong>-->
<!--                                <br><small>Robot follows human gaze to shared objects</small>-->
<!--                                <br><em>üìÑ From "Boundary Conditions for Human Gaze Estimation" paper</em>-->
<!--                            </div>-->
                            
                            <div class="demo-frame">
                                <video width="100%" height="280" controls style="border-radius: 10px;">
                                    <source src="videos/gaze-following-demo.mp4" type="video/mp4">
                                    <p>Your browser doesn't support video playback.</p>
                                </video>
                                <p style="text-align: center; margin-top: 10px; color: #666;">
                                    <strong>Live Gaze Tracking Demo</strong> - Real-time gaze detection on Pepper robot
                                </p>
                            </div>
                            
                            <div class="tech-tags">
                                <span class="tech-tag">L2CS Model</span>
                                <span class="tech-tag">Pepper Robot</span>
                                <span class="tech-tag">Gaze Estimation</span>
                                <span class="tech-tag">Shared Attention</span>
                            </div>
                            
                            <div class="project-links">
                                <!-- Links removed - replace with actual URLs when available -->
                            </div>
                        </div>
                    </div>

                    <div class="project-card">
                        <div class="project-header">
                            <h3 class="project-title">üìä Automatic Gaze Target Annotation</h3>
                        </div>
                        <div class="project-content">
                            <p class="project-description">Automated pipeline that identifies gaze targets in human-robot interaction videos to measure engagement. The system eliminates time-consuming manual annotation by automatically processing videos where objects remain static, significantly streamlining HRI research workflows.</p>
                            
<!--                            <div class="demo-frame">-->
<!--                                <strong>Automatic Annotation Pipeline</strong>-->
<!--                                <br><small>Automated gaze target identification in HRI videos</small>-->
<!--                                <br><em>üìà Published at RO-MAN 2024</em>-->
<!--                            </div>-->
                            
                            <div class="demo-frame">
                                <video width="100%" height="280" controls style="border-radius: 10px;">
                                    <source src="videos/Automated-annotation.mp4" type="video/mp4">
                                    <p>Your browser doesn't support video playback.</p>
                                </video>
                                <p style="text-align: center; margin-top: 10px; color: #666;">
                                    <strong>Live Gaze Tracking Demo</strong> - Real-time gaze detection on Pepper robot
                                </p>
                            </div>
                            
                            <div class="tech-tags">
                                <span class="tech-tag">Computer Vision</span>
                                <span class="tech-tag">Video Analysis</span>
                                <span class="tech-tag">Automation</span>
                                <span class="tech-tag">Engagement Metrics</span>
                            </div>
                            
                            <div class="project-links">
                                <!-- Links removed - replace with actual URLs when available -->
                            </div>
                        </div>
                    </div>

                    <div class="project-card">
                        <div class="project-header">
                            <h3 class="project-title">üîç Multi-modal Gaze Tracking in HRI</h3>
                        </div>
                        <div class="project-content">
                            <p class="project-description">Live interaction experiment using an incongruence paradigm to assess participants' comprehension of cues from different channels. Combined eye-tracking glasses with four L2CS model configurations to investigate visual attention across modalities and evaluate L2CS feasibility in HRI scenarios.</p>
<!--                            -->
<!--                            <div class="demo-frame">-->
<!--                                <strong>Multi-modal Interaction Study</strong>-->
<!--                                <br><small>Eye-tracking analysis across different modalities</small>-->
<!--                                <br><em>üèÜ Accepted at IROS 2025</em>-->
<!--                            </div>-->
                            
                            <div class="demo-frame">
                                <video width="100%" height="280" controls style="border-radius: 10px;">
                                    <source src="videos/Multi-modal-HRI.mp4" type="video/mp4">
                                    <p>Your browser doesn't support video playback.</p>
                                </video>
                                <p style="text-align: center; margin-top: 10px; color: #666;">
                                    <strong>Live Gaze Tracking Demo</strong> - Real-time gaze detection on Pepper robot
                                </p>
                            </div>
                            
                            <div class="tech-tags">
                                <span class="tech-tag">Eye-tracking Glasses</span>
                                <span class="tech-tag">L2CS Model</span>
                                <span class="tech-tag">Multi-modal</span>
                                <span class="tech-tag">Visual Attention</span>
                                <span class="tech-tag">Incongruence</span>
                            </div>
                            
                            <div class="project-links">
                                <!-- Links removed - replace with actual URLs when available -->
                            </div>
                        </div>
                    </div>

                    <div class="project-card">
                        <div class="project-header">
                            <h3 class="project-title">ü§ñ Gaze-Based Multi-Robot Control System</h3>
                        </div>
                        <div class="project-content">
                            <p class="project-description">Validation experiment for a gaze-based and haptic-feedback remote control system where an operator guided a multi-robot system through a 6m √ó 2m corridor. The robots responded to operator commands (speed, formation type, and size) while performing autonomous tasks including obstacle avoidance, collision prevention, and formation maintenance.</p>
                            
<!--                            <div class="demo-frame">-->
<!--                                <strong>Multi-Robot Control Validation</strong>-->
<!--                                <br><small>Gaze + haptic feedback for robot formation control</small>-->
<!--                                <br><em>üõ†Ô∏è Master's thesis validation experiment</em>-->
<!--                            </div>-->
<!--                            -->

                            <div class="demo-frame">
                                <video width="100%" height="280" controls style="border-radius: 10px;">
                                    <source src="videos/teleoperation.mp4" type="video/mp4">
                                    <p>Your browser doesn't support video playback.</p>
                                </video>
                                <p style="text-align: center; margin-top: 10px; color: #666;">
                                    <strong>Live Gaze Tracking Demo</strong> - Real-time gaze detection on Pepper robot
                                </p>
                            </div>
                            
                            
                            
                            <div class="tech-tags">
                                <span class="tech-tag">Multi-robot Systems</span>
                                <span class="tech-tag">Gaze Control</span>
                                <span class="tech-tag">Haptic Feedback</span>
                                <span class="tech-tag">Formation Control</span>
                                <span class="tech-tag">Autonomous Navigation</span>
                            </div>
                            
                            <div class="project-links">
                                <!-- Links removed - replace with actual URLs when available -->
                            </div>
                        </div>
                    </div>
                </div>
            </section>

            <section id="publications" class="section">
                <h2>Publications</h2>
                <div class="publications">
                    <div class="publication-item">
                        <div class="pub-title">Beware of the Tablet: A Dominant Distractor in Human-Robot Interaction</div>
                        <div class="pub-authors">L. Cheng, A.V. Belopolsky, M. de Bruijn, & K.V. Hindriks</div>
                        <div class="pub-venue">IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2025) <span class="pub-status">Accepted</span></div>
                    </div>
                    
                    <div class="publication-item">
                        <div class="pub-title">Evaluating Appearance-Based Gaze Pattern for Human-Robot Interaction</div>
                        <div class="pub-authors">L. Cheng, K.V. Hindriks, M. de Bruijn, & A.V. Belopolsky</div>
                        <div class="pub-venue">IEEE International Conference on Robot and Human Interactive Communication (RO-MAN 2025) <span class="pub-status">Accepted</span></div>
                    </div>
                    
                    <div class="publication-item">
                        <div class="pub-title">Automating Gaze Target Annotation in Human-Robot Interaction</div>
                        <div class="pub-authors">L. Cheng, K.V. Hindriks, & A.V. Belopolsky</div>
                        <div class="pub-venue">Proceedings of the 33rd IEEE International Conference on Robot and Human Interactive Communication (RO-MAN 2024) <span class="pub-status published">Published</span></div>
                    </div>
                    
                    <div class="publication-item">
                        <div class="pub-title">Boundary Conditions for Human Gaze Estimation on A Social Robot using State-of-the-Art Models</div>
                        <div class="pub-authors">L. Cheng, A.V. Belopolsky, & K.V. Hindriks</div>
                        <div class="pub-venue">Proceedings of the 32nd IEEE International Conference on Robot and Human Interactive Communication (RO-MAN 2023) <span class="pub-status published">Published</span></div>
                    </div>
                    
                    <div class="publication-item">
                        <div class="pub-title">A Gaze-based Bilateral Teleoperation Framework for a Team of Mobile Robots</div>
                        <div class="pub-authors">J. Mao, G. Song, L. Cheng, M. Zhang, S. Xie, S. Hao, & A. Song</div>
                        <div class="pub-venue">Proceedings of the IEEE International Conference on Robotics and Biomimetics (ROBIO 2023) <span class="pub-status published">Published</span></div>
                    </div>

                    <div class="publication-item">
                        <div class="pub-title">A Semi-autonomous Control Method for Formation and Navigation of Multiple Mobile Robots Based on Force Feedback and Eye Tracking</div>
                        <div class="pub-authors">G. Song & L. Cheng</div>
                        <div class="pub-venue">Chinese Patent No. CN110825076A <span class="pub-status published">Patent</span></div>
                    </div>
                </div>
            </section>

            <section id="contact" class="section">
                <h2>Get In Touch</h2>
                <div class="contact-grid">
                    <div class="contact-card">
                        <div class="contact-icon">üìß</div>
                        <h3>Email</h3>
                        <p>linlincheng.research@gmail.com</p>
                    </div>
                    
                    <div class="contact-card">
                        <div class="contact-icon">üêô</div>
                        <h3>GitHub</h3>
                        <p>github.com/LinReseach</p>
                    </div>
                    
                    <div class="contact-card">
                        <div class="contact-icon">üì±</div>
                        <h3>Phone</h3>
                        <p>+31 644600165</p>
                    </div>
                    
                    <div class="contact-card">
                        <div class="contact-icon">üèõÔ∏è</div>
                        <h3>Institution</h3>
                        <p>Vrije Universiteit Amsterdam</p>
                    </div>
                </div>
            </section>
        </main>

        <footer>
            <p>&copy; 2025 Linlin Cheng. All rights reserved. | PhD Researcher in Human-Robot Interaction | VU Amsterdam</p>
        </footer>
    </div>

    <script>
        // Smooth scrolling for navigation links
        document.querySelectorAll('nav a').forEach(link => {
            link.addEventListener('click', function(e) {
                e.preventDefault();
                const targetId = this.getAttribute('href');
                const targetSection = document.querySelector(targetId);
                
                // Update active nav link
                document.querySelectorAll('nav a').forEach(l => l.classList.remove('active'));
                this.classList.add('active');
                
                // Smooth scroll to section
                targetSection.scrollIntoView({
                    behavior: 'smooth',
                    block: 'start'
                });
            });
        });

        // Add some interactive demo placeholders
        document.addEventListener('DOMContentLoaded', function() {
            // Simulate loading stats with animation
            const statNumbers = document.querySelectorAll('.stat-number');
            statNumbers.forEach(stat => {
                const finalValue = stat.textContent;
                if (finalValue.includes('.')) {
                    // Handle decimal numbers like 18.6%
                    const numericValue = parseFloat(finalValue);
                    let currentValue = 0;
                    const increment = numericValue / 50;
                    
                    const timer = setInterval(() => {
                        currentValue += increment;
                        if (currentValue >= numericValue) {
                            currentValue = numericValue;
                            clearInterval(timer);
                        }
                        stat.textContent = currentValue.toFixed(1) + (finalValue.includes('%') ? '%' : '');
                    }, 30);
                } else if (!isNaN(parseInt(finalValue))) {
                    // Handle regular numbers
                    const numericValue = parseInt(finalValue);
                    let currentValue = 0;
                    const increment = numericValue / 50;
                    
                    const timer = setInterval(() => {
                        currentValue += increment;
                        if (currentValue >= numericValue) {
                            currentValue = numericValue;
                            clearInterval(timer);
                        }
                        stat.textContent = Math.floor(currentValue) + (finalValue.includes('+') ? '+' : '');
                    }, 30);
                }
            });

            // Add click handlers for demo buttons
            document.querySelectorAll('.btn-primary').forEach(btn => {
                btn.addEventListener('click', function(e) {
                    e.preventDefault();
                    const projectTitle = this.closest('.project-card').querySelector('.project-title').textContent;
                    alert(`Demo for "${projectTitle}" would launch here! Replace with your actual project demos.`);
                });
            });

            // Add GitHub link functionality
            document.querySelectorAll('.btn-secondary').forEach(btn => {
                btn.addEventListener('click', function(e) {
                    if (this.textContent.includes('GitHub')) {
                        window.open('https://github.com/LinReseach', '_blank');
                    } else {
                        e.preventDefault();
                        alert('Link to paper/patent would go here!');
                    }
                });
            });
        });

        // Intersection Observer for scroll animations
        const observerOptions = {
            threshold: 0.1,
            rootMargin: '0px 0px -50px 0px'
        };

        const observer = new IntersectionObserver((entries) => {
            entries.forEach(entry => {
                if (entry.isIntersecting) {
                    entry.target.style.opacity = '1';
                    entry.target.style.transform = 'translateY(0)';
                }
            });
        }, observerOptions);

        document.querySelectorAll('.section').forEach(section => {
            observer.observe(section);
        });

        // Update active nav on scroll
        window.addEventListener('scroll', () => {
            const sections = document.querySelectorAll('.section');
            const navLinks = document.querySelectorAll('nav a');
            
            let current = '';
            sections.forEach(section => {
                const sectionTop = section.offsetTop;
                const sectionHeight = section.clientHeight;
                if (scrollY >= (sectionTop - 200)) {
                    current = section.getAttribute('id');
                }
            });

            navLinks.forEach(link => {
                link.classList.remove('active');
                if (link.getAttribute('href') === `#${current}`) {
                    link.classList.add('active');
                }
            });
        });
